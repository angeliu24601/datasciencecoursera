## 1. --------------------------------------------------------------------------
## Load the vowel.train and vowel.test data sets:
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
vowwl.train$y <- factor(vowwl.train$y)
vowel.train$y <- factor(vowel.train$y)
mod1 <- train(y ~ ., method = "rf", data = vowel.train)
## What are the accuracies for the two approaches on the test data set?
## What is the accuracy among the test set samples where the two methods agree?
library(caret)
mod1 <- train(y ~ ., method = "rf", data = vowel.train)
mod2 <- train(y ~ ., method = "gbm", data = vowel.train)
accuracy
pred1 <- predict(mod1, newdata = vowel.test)
library(SDMTools)
install.packages("SDMTools")
library(SDMTools)
accuracy(vowel.test, pred1)
accuracy(vowel.test$y, pred1)
?accuracy
?accuracy
?confusion.matrix
confusionMatrix()
?confusionMatrix()
confusionMatrix(predi1, vowel.test$y)
confusionMatrix(pred1, vowel.test$y)
vowel.test$y <- factor(vowel.test$y)
confusionMatrix(pred1, vowel.test$y)
confusionMatrix(pred1, vowel.test$y)$Overall
confusionMatrix(pred1, vowel.test$y)$overall
confusionMatrix(pred1, vowel.test$y)$overall[1]
pred2 <- predict(mod2, newdata = vowel.test)
confusionMatrix(pred2, vowel.test$y)$overall[1]
df <- data.frame(vowel.test$y, pred1, pred2)
View(df)
df.agree <- data.frame(vowel.test$y, pred1, pred2) %>%
filter(pred1 == pred2)
library(tidyverse)
df.agree <- data.frame(vowel.test$y, pred1, pred2) %>%
filter(pred1 == pred2)
View(df.agree)
confusion.matrix(pred1, df.agree$vowel.test.y)$overall[1]
rm(df)
View(df.agree)
confusionMatrix(pred1, df.agree$vowel.test.y)$overall[1]
confusionMatrix(df.agree$pred1, df.agree$vowel.test.y)$overall[1]
confusionMatrix(pred2, vowel.test$y)$overall[1]
confusionMatrix(df.agree$pred1, df.agree$vowel.test.y)$overall[1]
confusionMatrix(df.agree$pred1, df.agree$vowel.test.y)$overall[1]
confusionMatrix(pred1, vowel.test$y)$overall[1]
confusionMatrix(pred2, vowel.test$y)$overall[1]
confusionMatrix(df.agree$pred1, df.agree$vowel.test.y)$overall[1]
## 2. -------------------------------------------------------------------------
## Load the Alzheimer's data using the following commands
library(caret)
library(gbm)
rm(list=ls())
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
## Set the seed to 62433 and predict diagnosis with all the other variables
## using a random forest ("rf"), boosted trees ("gbm") and linear discriminant
## analysis ("lda") model. Stack the predictions together using random forests
## ("rf"). What is the resulting accuracy on the test set?
## Is it better or worse than each of the individual predictions?
set.seed(62433)
View(training)
mod1 <- train(diagnosis ~., method = "rf",  data = training)
mod2 <- train(diagnosis ~., method = "gbm", data = training)
mod3 <- train(diagnosis ~., method = "lda", data = training)
df.stack <- data.frame(testing$diagnosis,
rf = predict(mod1, newdata = testing))
View(df.stack)
df.stack <- data.frame(testing$diagnosis,
rf = predict(mod1, newdata = testing),
gbm = predict(mod2, newdata = testing),
lda = predict(mod3, newdata = testing),)
df.stack <- data.frame(testing$diagnosis,
rf = predict(mod1, newdata = testing),
gbm = predict(mod2, newdata = testing),
lda = predict(mod3, newdata = testing))
mod4 <- train(diagnosis ~., method = "rf", data = df.stack)
df.stack <- data.frame(diagnosis = testing$diagnosis,
rf = predict(mod1, newdata = testing),
gbm = predict(mod2, newdata = testing),
lda = predict(mod3, newdata = testing))
mod4 <- train(diagnosis ~., method = "rf", data = df.stack)
View(mod4)
View(mod4)
confusionMatrix(mod1)$overall[1]
confusionMatrix(mod1, testing$diagnosis)$overall[1]
confusionMatrix(df.stack$rf, testing$diagnosis)$overall[1]
confusionMatrix(df.stack$gbm, testing$diagnosis)$overall[1]
confusionMatrix(df.stack$lda, testing$diagnosis)$overall[1]
confusionMatrix(predict(mod4, newdata = testing), testing$diagnosis)$overall[1]
confusionMatrix(df.stack$lda, testing$diagnosis)$overall[1]
predict(mod4, newdata = testing)
confusionMatrix(predict(mod4, newdata = df.stack$diagnosis),
testing$diagnosis)$overall[1]
confusionMatrix(predict(mod4, newdata = df.stack),
testing$diagnosis)$overall[1]
rm(list = ls())
## 3. --------------------------------------------------------------------------
## Load the concrete data with the commands:
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
set.seed(233)
mod <- train(CompressiveStrength ~., method = "lasso", data = training)
?plot.enet
plot.enet(mod$finalModel, xvar = "penalty")
library(elasticnet)
plot.enet(mod$finalModel, xvar = "penalty")
## 4. --------------------------------------------------------------------------
## Load the data on the number of visitors to the instructors blog from here:
## https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv
## Using the commands:
##
library(lubridate) # For year() function below
rm(list=ls())
dat = read.csv("~/Desktop/gaData.csv")
download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/gaData.csv",
"./gaData.csv")
dat = read.csv("~/gaData.csv")
dat = read.csv("./gaData.csv")
training = dat[year(dat$date) < 2012,]
testing = dat[(year(dat$date)) > 2011,]
tstrain = ts(training$visitsTumblr)
## Fit a model using the bats() function in the forecast package to the training
## time series. Then forecast this model for the remaining time points.
## For how many of the testing points is the true value within the 95% prediction
## interval bounds?
library(forest)
## Fit a model using the bats() function in the forecast package to the training
## time series. Then forecast this model for the remaining time points.
## For how many of the testing points is the true value within the 95% prediction
## interval bounds?
library(forcast)
install.packages("forecast")
?bats()
library(forecast)
?bats
tstrain = ts(training$visitsTumblr)
mod <- bats(tstrain)
plot(mod)
?forecast
fcast <- forecast(mod, level = 95)
dim(testing)
fcast$lower
fcast <- forecast(mod, level = 95, h = dim(testing)[1])
fcast$lower
df <- data.frame(lower = fcast$lower, upper = fcast$upper, val = testing$visitsTumblr)
View(df)
df <- data.frame(lower = fcast$lower, upper = fcast$upper,
val = testing$visitsTumblr) %>%
mutate(within = ifelse(val >= lower & val <= upper, TRUE, FALSE))
fcast$lower$`95%`
class(fcast$lower)
as.numeric(fcast$lower)
df <- data.frame(lower = as.numeric(fcast$lower),
upper = as.numeric(fcast$upper),
val = testing$visitsTumblr) %>%
mutate(within = ifelse(val >= lower & val <= upper, TRUE, FALSE))
sum(df$within)/nrow(df)
rm(list = ls())
## 5. --------------------------------------------------------------------------
## Load the concrete data with the commands:
set.seed(3523)
library(AppliedPredictiveModeling)
data(concrete)
inTrain = createDataPartition(concrete$CompressiveStrength, p = 3/4)[[1]]
training = concrete[ inTrain,]
testing = concrete[-inTrain,]
## Set the seed to 325 and fit a support vector machine using the e1071 package
## to predict Compressive Strength using the default settings.
## Predict on the testing set. What is the RMSE?
library(e1071)
set.seed(325)
?svm
mod <- svm(CompressiveStrength ~., data = training)
prd <- predict(mod, newdata = testing)
RMSE(prd, testing$CompressiveStrength)
## Set the seed to 62433 and predict diagnosis with all the other variables
## using a random forest ("rf"), boosted trees ("gbm") and linear discriminant
## analysis ("lda") model. Stack the predictions together using random forests
## ("rf"). What is the resulting accuracy on the test set?
## Is it better or worse than each of the individual predictions?
set.seed(62433)
## Set the seed to 62433 and predict diagnosis with all the other variables
## using a random forest ("rf"), boosted trees ("gbm") and linear discriminant
## analysis ("lda") model. Stack the predictions together using random forests
## ("rf"). What is the resulting accuracy on the test set?
## Is it better or worse than each of the individual predictions?
set.seed(62433)
mod1 <- train(diagnosis ~., method = "rf",  data = training)
mod2 <- train(diagnosis ~., method = "gbm", data = training)
mod3 <- train(diagnosis ~., method = "lda", data = training)
## 2. -------------------------------------------------------------------------
## Load the Alzheimer's data using the following commands
library(caret)
library(gbm)
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
## Set the seed to 62433 and predict diagnosis with all the other variables
## using a random forest ("rf"), boosted trees ("gbm") and linear discriminant
## analysis ("lda") model. Stack the predictions together using random forests
## ("rf"). What is the resulting accuracy on the test set?
## Is it better or worse than each of the individual predictions?
set.seed(62433)
mod1 <- train(diagnosis ~., method = "rf",  data = training)
mod2 <- train(diagnosis ~., method = "gbm", data = training)
mod3 <- train(diagnosis ~., method = "lda", data = training)
df.stack <- data.frame(diagnosis = testing$diagnosis,
rf = predict(mod1, newdata = testing),
gbm = predict(mod2, newdata = testing),
lda = predict(mod3, newdata = testing))
mod4 <- train(diagnosis ~., method = "rf", data = df.stack)
confusionMatrix(df.stack$rf, testing$diagnosis)$overall[1]
confusionMatrix(df.stack$gbm, testing$diagnosis)$overall[1]
confusionMatrix(df.stack$lda, testing$diagnosis)$overall[1]
confusionMatrix(predict(mod4, newdata = df.stack),
testing$diagnosis)$overall[1]
